<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Duckiebot Exercise Deliverables</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 0;
            padding: 0;
        }

        header, footer {
            background-color: #f4f4f4;
            padding: 10px 0;
        }

        section {
            margin: 20px auto;
            width: 80%;
        }

        img {
            max-width: 70%;
            height: auto;
            display: block;
            margin: 10px auto;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        iframe {
            width: 80%;
            height: 450px;
            margin: 20px 0;
            border: none;
        }

        h1, h2, h3 {
            color: #333;
        }

        p, ul {
            text-align: justify;
        }

        ul {
            list-style-type: disc;
            margin: 10px 20px;
            padding-left: 20px;
        }

        footer {
            font-size: 0.9em;
            color: #666;
        }

        .references {
            margin: 20px auto;
            width: 80%;
            text-align: left;
        }

        .references ul {
            list-style-type: none;
            padding-left: 0;
        }

        .references li {
            margin: 5px 0;
        }
    </style>
</head>
<body>
    <header>
        <h1>Duckiebot Exercise Deliverables</h1>
    </header>

    <section>
        <h2>Deliverables</h2>
        <article>
            <h3>Part One - AprilTag Detection</h3>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/ufpAvzWERo8"
  frameborder="0"
  allowfullscreen>
</iframe>
            <p>Screen recording of the rqt_image_view showing the image augmentations applied when detecting AprilTags

This video shows the Duckiebot detecting the closest AprilTag, drawing a box around it, and displaying its corresponding id number</p>
        </article>

        <article>
            <h3>Image Processing Steps</h3>
            <ul>
                <li>Image Conversion from CompressedImage to OpenCV Format
Justification: The incoming image is in a compressed format (ROS CompressedImage), so it must be converted to an OpenCV-compatible format (e.g., a NumPy array) for further processing.</li>
                <li>Image Undistortion
Justification: Camera lenses introduce distortion, which can affect the accuracy of AprilTag detection. Undistorting the image using the camera's intrinsic parameters ensures geometric correctness, improving detection reliability.</li>
                <Li>Grayscale Conversion
Justification: AprilTag detection works on grayscale images. Converting the image to grayscale simplifies processing and reduces computational overhead, as color information is unnecessary for detection.</Li>
                <li>AprilTag Detection
Justification: This step detects AprilTags in the grayscale image using the dt_apriltags library. It provides tag ID, corners, and pose information, essential for the node's functionality.</li>
                <li>Closest Tag Selection
Justification: If multiple tags are detected, the node selects the closest one based on the Euclidean distance of the translation vector. This ensures the robot focuses on the most relevant tag in its vicinity.</li>
            </ul>
        </article>

        <article>
            <h3>Justification of your apriltag detection rate</h3>
            <h4>Image Acquisition</h4>
            <ul>
                <li>The node subscribes to a ROS topic (/camera_node/image/compressed) to receive compressed images from the camera. This step captures the raw image data from the camera, which serves as the input for AprilTag detection.</li>
            </ul>
            <h4>Image Conversion</h4>
            <ul>
                <li>The compressed image is converted from a ROS CompressedImagemessage to an OpenCV-compatible format (e.g., a NumPy array) using the cv_bridge library. OpenCV is used for image processing, so the image must be in a format that OpenCV can manipulate.</li>
            </ul>
            <h4>Image Undistortion</h4>
            <ul>
                <li>The image is undistorted using the camera's intrinsic parameters (camera_matrix and distortion_coeffs), which are obtained from the ROS CameraInfo message. Camera lenses introduce distortion, which can affect the accuracy of AprilTag detection. Undistorting the image corrects these distortions, ensuring that the image is geometrically accurate.</li>
            </ul>
            <h4>Grayscale Conversion</h4>
            <ul>
                <li>The undistorted image is converted from color (BGR) to grayscale using OpenCV's cvtColor function. AprilTag detection works on grayscale images, so this step simplifies the image data and reduces computational overhead.</li>
            </ul>
            <h4>AprilTag Detection</h4>
            <ul>
                <li>The grayscale image is passed to the dt_apriltags detector, which performs the following steps:</li>
                <li>Edge Detection: The detector identifies edges in the image to locate potential tag candidates.</li>
                <li>Quad Decoding: It decodes the detected quads (four-sided polygons) to determine if they correspond to valid AprilTags.</li>
                <li>Tag Decoding: The detector extracts the binary payload from the tag and matches it against known tag families (e.g., tag36h11).</li>
                <li>Pose Estimation: For each detected tag, the detector estimates its 3D pose (position and orientation) relative to the camera using the camera's intrinsic parameters and the known tag size.</li>
                <li>This step identifies the tags in the image, extracts their IDs, and estimates their positions in the environment.</li>
            </ul>
            <h4>Closest Tag Selection</h4>
            <ul>
                <li>Process: If multiple tags are detected, the node calculates the Euclidean distance of each tag's translation vector (pose_t) and selects the closest one.</li>
                <li>Purpose: This ensures that the robot focuses on the most relevant tag in its immediate vicinity, which is useful for navigation or interaction tasks.</li>
            </ul>
            <h4>Image Augmentation</h4>
            <ul>
                <li>Process: The node augments the original image by drawing bounding boxes and tag IDs for the detected tags using OpenCV functions like cv2.line and cv2.putText.</li>
                <li>Purpose: This step provides visual feedback, making it easier to debug and verify the detection results.</li>
            </ul>
            <h4>Publishing Results</h4>
            <ul>
                <li>Process: The augmented image is published as a ROS Image message for visualization, and the detected tag ID is published as a ROS Int32 message for use by other nodes.</li>
                <li>Purpose: This allows other parts of the system (e.g., navigation or control nodes) to use the detection results.</li>
            </ul>
        </article>

        <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/AlwnRQhs8Wc"
  frameborder="0"
  allowfullscreen>
</iframe>
            <p>A video showing the bot driving straight for at least 30cm. Detecting the “Stop Sign” apriltag and changing LEDs to red. Stopping before the red line for 3 seconds. Moving straight past the intersection.</p>
        </article>
        <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/_gEnwDfDm2o"
  frameborder="0"
  allowfullscreen>
</iframe>
            <p>A video showing the bot driving straight for at least 30 cm. Detecting the “T -Intersection” apriltag and changing LEDs to Blue. Stopping before the red line for 2 seconds. Moving straight past the intersection.</p>
        </article>
        <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/HaTuBxjgRpw"
  frameborder="0"
  allowfullscreen>
</iframe>
            <p>A video showing the bot driving straight for at least 30cm. Detecting the “UofA Tag” apriltag and changing LEDs to Green. Stopping before the red line for 1 second. Moving straight past the intersection.</p>
        </article>
        <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/NhVYSsJ9LbQ"
  frameborder="0"
  allowfullscreen>
</iframe>
            <p>A video showing the bot driving straight for at least 30 cm. Detecting no apriltags and keeping LEDs white. Stopping before the red line for 0.5 seconds. Moving straight past the intersection.</p>
            <p>Explanation of how the apriltag detection rate affects the intersection detection: The AprilTag detection rate of 3 detections per second affects intersection detection in the following ways:</p>
            <ul>
                <li>Latency: Introduces a small delay between detections, which could be problematic for fast-moving robots.</li>
                <li>Responsiveness: Limits the system's ability to react quickly to dynamic changes</li>
            </ul>
            <p>For most practical applications (e.g., slow-moving robots in controlled environments), a detection rate of 3 FPS is sufficient for reliable intersection detection. However, for faster robots or more demanding scenarios, a higher detection rate would improve performance.</p>
        </article>
                <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/SApx1KyqLGo"
  frameborder="0"
  allowfullscreen>
</iframe>
                    <h3>PeDuckstrian Crosswalks</h3>
            <p>The video on the left shows the behavior of our Duckiebot upon encountering peDuckstrian crosswalks. Upon encountering an empty crosswalk, our Duckiebot stops for one second before proceeding. Upon encountering a crosswalk with peDuckstrians, our Duckiebot waited until all the peDuckstrian finished crossing before driving past the crosswalk.</p>
        </article>

        <article>
            <h3>Dealing with Double Blue Lines and Detecting PeDuckstrians</h3>
            <h4>Empty crosswalk</h4>
            <img src="image/empty_crosswalk.png" alt="Screenshot of extrinsics calibration.yaml">
            <p>The image on the left shows our Duckiebot detecting the two blue lines of a crosswalk. We consider an empty crosswalk to be detected when our Duckiebot:</p>
            <ul>
                <li>detects at least two blue lines, and</li>
                <li>does not detect any peDuckstrians.</li>
            </ul>
            <p>The warping at the bottom of the image occasionally causes the Duckiebot to detect the bottom blue line as multiple blue lines. Therefore, we do not impose the stricter condition of requiring exactly two blue lines to be detected (and use condition 1 instead). Another method we tried was to crop the lower portion of the image (the warped part) and detect crosswalks on the cropped image (the unwarped part). Empirically, both methods worked well independently, but we combined them since they did not interfere with each other.
Our Duckiebot can encounter an empty crosswalk in two different ways:</p>
            <ul>
                <li>It drove up to an already empty crosswalk.</li>
                <li>It stopped before a crosswalk with peDuckstrians but the peDuckstrians have finished crossing.</li>
            </ul>
            <p>It is important to distinguish between the two situations because the Duckiebot behaves differently in the two situations.</p>
        </article>

        <article>
            <h3>Crosswalk with peDuckstrians</h3>
            <img src="image/crosswalk_with.png" alt="Screenshot of Duckiebot saying 'Hello from MY_ROBOT'">
            <p>The image on the left shows our Duckiebot detecting one of the blue lines of a crosswalk and the peDuckstrians on the crosswalk. We consider a crosswalk with peDuckstrians to be detected when our Duckiebot detects:</p>
            <ul>
                <li>at least one blue line (the peDuckstrians obstruct the blue line that is farther away), and</li>
                <li>at least one peDuckstrian.</li>
            </ul>
            <p>To detect peDuckstrians, we boldly assumed that they are (mostly) orange. So we simply detect if the camera is capturing any orange objects by specifying a mask for the color orange using an HSV range. This simple and crude strategy is (surprisingly) very effective for this assignment, but any orange objects will be detected as peDuckstrians.
</p>
        </article>

        <article>
            <h3>No crosswalks</h3>
            <img src="image/no_crosswalk.png" alt="Screenshot of Duckiebot saying 'Hello from MY_ROBOT'">
            <p>The image on the left shows our Duckiebot not detecting any crosswalks. Although an empty crosswalk and a crosswalk with peDuckstrians are seen in the image, they are too far away and thus not detected by our Duckiebot.</p>
        </article>

        <article>
            <h3>Crosswalk State Machine</h3>
             <p>We implemented the behavior of our Duckiebot upon encountering a crosswalk using a 4-stage state machine (shown below).</p>
            <img src="image/State_Machine.jpg" alt="Screenshot of Duckiebot saying 'Hello from MY_ROBOT'">
            <p>Our crosswalk state machine has 4 stages:</p>
            <ul>
                <li>Driving normally.</li>
                <li>Driving across the crosswalk.</li>
                <li>Detected an empty crosswalk.</li>
                <li>Detected a crosswalk with peDuckstrians.</li>
            </ul>
            <p>When we are driving normally, we can either (1) drive up to and detect a crosswalk with peDuckstrians (stage 3), or (2) drive up to and detect an empty crosswalk (stage 2). If we detect a crosswalk with peDuckstrians, we must wait until all the peDuckstrians have crossed, after which we will detect an empty crosswalk (transition from stage 3 to stage 2). After checking that the crosswalk is clear, we proceed to drive across the crosswalk (transition from stage 2 to stage 1). While in stage 1, we will drive until we detect no more blue lines, after which we can be sure that we have cross the crosswalk, and can return to driving normally (transition from stage 1 to stage 0).</p>
            <p>To summarize, we had to handle the double blue lines in two cases: detection, and, thereafter, the behaviour.</p>
            <p>We detect the crosswalk in two situations:</p>
            <ul>
                <li>An empty crosswalk ⇔ at least two blue lines.</li>
                <li>Due to the warping of the lower portion of the image (even after undistortion), the closer of the two blue lines may be detected as two blue lines. Therefore, we impose a more relaxed condition on the number of detected blue lines (at least two instead of exactly two).</li>
                <li>A crosswalk with peDuckstrians ⇔ peDuckstrians and at least one blue line.</li>
                <li>We used an orange colour mask defined using an HSV range to detect the peDuckstrians. Very simple to implement, but anything orange will be (falsely) detected as a peDuckstrian.</li>
                <li>The peDuckstrians may obstruct the blue line that is farther away, causing the Duckiebot not to detect it. Therefore, we impose an even more relaxed condition on the number of detected blue lines (at least one instead of at least two as for an empty crosswalk).</li>
            </ul>
            <p>We implemented a 4-stage state machine that controls our Duckiebot's behaviour upon encountering a crosswalk:</p>
            <ul>
                <li>Drive until we detect a crosswalk and stop before the crossswalk.</li>
                <li>After detecting no peDuckstrians, start driving, but ignore the detected blue lines.</li>
                <li>After driving across the crosswalk, we will not detect blue lines anymore, but we will start detecting blue lines again (so we can detect new crosswalks).</li>
            </ul>
        </article>
        <article>
            <iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/xSQgFuj524A"
  frameborder="0"
  allowfullscreen>
</iframe>
                    <h3>Safe Avoidance Navigation</h3>
            <p>The video on the left shows the behavior of our Duckiebot encountering a broken bot on the lane. Upon encountering the broken-down bot, our Duckiebot stops for three seconds before proceeding. Next, it will run lane change procedures to avoid and overpass the broken bot in which in will continue lane following.</p>
        </article>

        <article>
            <h3>Method Used for Detection</h3>
            <img src="image/detect_vehicle.png" alt="Screenshot of Duckiebot saying 'Hello from MY_ROBOT'">
            <p>For detecting the broken-down Duckiebot, we used a method where the Duckiebot detects the grid-like dots on the back of the other Duckiebot. These dots are a distinctive feature and were easily recognizable by the Duckiebot's front-facing camera. We borrowed an existing implementation from the Duckiebot GitHub repository that was designed to recognize these grid-like patterns.</p>
            <p>The grid pattern detection was based on visual features, where the camera captured the image and applied computer vision techniques to identify the dots. Once the dots were detected, the Duckiebot knew that it was approaching another bot and took the necessary actions to stop and assess the situation.</p>
            <p>Other Methods Tried:</p>
            <p>Blue Color Detection: Initially, we considered using blue color detection to identify the Duckiebot, since the bodies of Duckiebots are typically blue. However, we quickly realized this method could be inconsistent because crosswalk lines are also blue. This would lead to false positives and make it difficult for the Duckiebot to reliably distinguish between the Duckiebot and other blue-colored objects like crosswalks. Because of this, we decided to focus on the grid pattern detection as the more consistent method.</p>
        </article>
        <article>
            <h3>Maneuver</h3>
            <img src="image/maneuver.png" alt="Screenshot of Duckiebot saying 'Hello from MY_ROBOT'">
            <p>To maneuver around the broken-down Duckiebot, we borrowed code from Exercise 2, which involved making 90-degree turns and moving in a straight line. The following sequence was used:</p>
            <p>Turn into the Opposing Lane: After pausing for 3 seconds, the Duckiebot uses the 90-degree turn algorithm to turn into the opposing lane. This maneuver ensures that the bot avoids the broken Duckiebot while maintaining control of its path.</p>
            <p>Straight Line Maneuver: After making the 90-degree turn, the Duckiebot moves in a straight line for a safe distance before returning to the proper lane. This allows the bot to pass the stationary Duckiebot without making contact.</p>
        </article>

    </section>

    <section>
        <h2>Reflection</h2>
        <h3>What we implemented</h3>
        <p>
            In this exercise, we focused on implementing perception-based safety mechanisms for mobile robots using Apriltag detection and obstacle avoidance strategies. We integrated computer vision techniques to detect Apriltags, recognize crosswalks, and maneuver safely around obstacles. Additionally, we explored methods for detecting a stationary Duckiebot and executing a safe navigation strategy.        </p>
        <h3>How it works</h3>
        <ul>
            <li>Apriltag Detection: We used the Apriltag library to detect tags in the environment, applying image preprocessing steps such as undistortion, grayscale conversion, and thresholding to improve detection accuracy.</li>
            <li>Apriltag-Based Actions: Upon detecting an Apriltag, the robot responded by changing its LED colour and stopping for a predefined time based on the tag type.</li>
            <Li>Crosswalk Detection & PeDuckstrians: The system detects peDuckstrians as orange objects. If the robot detects no peDuckstrians and at least two blue lines, then the robot is detecting a crosswalk with peDuckstrians, and has to wait until all the peDuckstrians are finished crossing. If the robot detects only blue lines and there are at least two of them, then the robot is detecting an empty crosswalk, where it has to wait at least one second before proceeding.  </Li>
            <li>Obstacle Avoidance: A broken-down Duckiebot was detected using visual cues, prompting our robot to stop, assess the situation, and carefully navigate around it before resuming lane following.</li>
            <li>Safe Maneuvering: The robot transitioned smoothly back into its designated lane after successfully avoiding the obstacle.</li>
        </ul>
         <h3>What challenges we came across</h3>
        <ul>
            <li>Noisy AprilTag/lane Detections: Variability in lighting sometimes leads to the duckiebot detecting the base of the AprilTag stand as a white lane</li>
            <li>Multiple AprilTags Confusion: When multiple AprilTags are in the image, duckiebot behaves unpredictably</li>
            <Li>Crosswalk Double Line Detection: Warping at the bottom of the image occasionally causes the closer blue line to be detected as two blue lines.</Li>
            <li>Crosswalk Behaviour: Implementing the behaviour of our robot at crosswalks needs extra care, so we don't drive across an empty crosswalk without stopping, nor drive over the peDuckstrians.</li>
            <li>Obstacle Maneuvering: Designing a robust avoidance maneuver without overshooting or colliding with obstacles requires extensive fine-tuning.</li>
            <li>Processing Delays: Handling multiple vision-based tasks simultaneously introduced minor lag, affecting the real-time response of the robot.</li>
        </ul>
        <h3>How we overcame the challenges</h3>
        <ul>
            <li>Refining AprilTag Processing A: We cropped the image and adjusted HSV thresholds to improve robustness under different lighting conditions.</li>
            <Li>Refining AprilTag Processing B: Only detect the closest AprilTag when multiple are in the image</Li>
            <li>Prevent Warped Crosswalks: Cropped the bottom portion of the image and used the remainder of the image (which is undistorted) for crosswalk detection.</li>
            <Li>4-stage state machine: Introduced a state machine that logically separated the crosswalk behaviour into four hierarchical stages.</Li>
            <li>Incremental Maneuver Testing: We systematically adjusted turning angles and stopping distances to achieve a safe and efficient avoidance path.</li>
            <li>Performance Optimization: Reduced computational overhead by optimizing vision processing pipelines, ensuring quicker decision-making.</li>
        </ul>
    </section>

    <section class="references">
        <h2>References</h2>
        <ul>
            <li><a href="https://github.com/duckietown/dt-core/blob/daffy/packages/duckietown_demos/launch/multi_lane_following.launch" target="_blank">https://github.com/duckietown/dt-core/blob/daffy/packages/duckietown_demos/launch/multi_lane_following.launch</a></li>
            <li><a href="https://github.com/duckietown/dt-core/blob/cc437e834227af82ab5530bcc7e1e6020147ab95/packages/vehicle_detection/src/vehicle_detection_node.py" target="_blank">https://github.com/duckietown/dt-core/blob/cc437e834227af82ab5530bcc7e1e6020147ab95/packages/vehicle_detection/src/vehicle_detection_node.py</a></li>
            <li><a href="https://github.com/duckietown/dt-core/blob/daffy/packages/apriltag/src/apriltag_detector_node.py" target="_blank">https://github.com/duckietown/dt-core/blob/daffy/packages/apriltag/src/apriltag_detector_node.py</a></li>

            <li>Special thanks to Zhaoyu Li and Yushu Zhang, who were my partners for this exercise. </li>
        </ul>
    </section>
</body>
</html>
